# Session Notes - Concept Tokenizer Project

## Last Updated: 2026-01-05

## Current State

### Completed
- **67 languages imported** from Kaikki with 7,996,091 compositions
- All scripts use hardcoded paths: `/usr/share/databases/`
- Spelling DB schema designed (`db/SCHEMA-spelling.sql`)
- Spelling DB builder created (`scripts/build_spelling_db.py`)

### Pending Tasks
1. **Build spelling DBs** - Run `build_spelling_db.py --lang en` (needs pyspellchecker, metaphone)
2. **Fix proper names extraction** - Module import path issue with token_encoder
3. **System primitives (00 range)** - Not yet implemented
4. **Legacy Mistral tokenizer mapping** - Future task

### Key Path Fix
All scripts hardcode `/usr/share/databases/` because `Path(__file__).resolve()`
follows symlinks to the repo path, breaking relative lookups.

### Genomic Token Format
`A.D.C.FF.SS.LLL.DD.FP.COL`
- A = Abstraction (0-99, proper names at 99)
- D = Domain (0=core, 1=physical, 10=entity)
- C = Category within domain
- FF.SS.LLL.DD = Language coordinates
- FP = Fingerprint (compositional hash)
- COL = Collision disambiguator

### Important Context
- Meeting with Alberta Shadow Minister for Tech/Innovation in ~12 days
- Goal: Demonstrate efficient AI on commodity hardware vs GPU farms
- User: Fast touch-typist, hands independent of processing (spelling correction relevant)

### User Preferences
- Hardcode paths, don't overthink dynamic resolution
- Keep it simple, practical
- Setting up `language_claude` user on physics machine for direct access

## Files Structure
```
/usr/share/databases/
├── db/
│   ├── lang/           # Per-language DBs (67 total)
│   ├── primitives.db   # NSM primitives
│   ├── spelling/       # Spelling correction DBs (to build)
│   └── *.sql           # Schema files
├── scripts/            # Import/build scripts
└── reference/
    └── kaikki/         # Source JSONL files
```
